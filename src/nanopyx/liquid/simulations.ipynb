{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Simulator import Method, Simulator, ALL_GEARS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of the game\n",
    " 1. We simulate a workflow with 3 methods\n",
    " 2. Method #1 runs faster in OpenCL implementations\n",
    " 3. Method #2 runs faster in Threaded implementations\n",
    " 4. Method #3 runs the same speed wherever\n",
    " 5. Each method has 9 implementations and each implementation takes a different time to run\n",
    " 6. We know a priori the REAL time distributions for each implementation and they are all Normal\n",
    " 7. The chance that an implementation is chosen is based upon the squared inverse of the past average values that were seen\n",
    " 8. Exponential weighting gives more importance to EARLIER benchmarks when calculating the past average value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case study #0 : Initialization of the agent / engine\n",
    "In this case we have minimal information.\n",
    "We only have information of one benchmark (aka all implementations were run once)\n",
    "\n",
    "We run the workflow 1000 times to calculate what is the average run time of the entire workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH NORMAL AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3)\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "print(\"AFTER ONE BENCHMARK\")\n",
    "sim.print_methods()\n",
    "\n",
    "print(\"AFTER SIMULATING\")\n",
    "sim.run_simulations(iter_n=1000)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH EXPONENTIALLY WEIGHTED AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1,exp=True)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2,exp=True)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3,exp=True)\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "sim.print_methods()\n",
    "\n",
    "sim.run_simulations(iter_n=1000)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case study #1 : Initialization of the engine with an EARLY and VERY BRIEF anomaly on ONE device (GPU) \n",
    "\n",
    "We start with information of one benchmark (aka all implementations were run once)\n",
    "\n",
    "We run the workflow 100 times. \n",
    "\n",
    "During 10-20 runs there is an anomaly with one GPU which affects all opencl implementation that use that GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH NORMAL AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3)\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "sim.print_methods()\n",
    "\n",
    "sim.run_anomalous_simulations(iter_n=100, ano_start=10, ano_end=20, affected_gear=ALL_GEARS[0], new_avg=4, new_std=0.4)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH EXPONENTIALLY WEIGHTED AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1,exp=True)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2,exp=True)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3,exp=True)\n",
    "\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "sim.print_methods()\n",
    "\n",
    "sim.run_anomalous_simulations(iter_n=100, ano_start=10, ano_end=20, affected_gear=ALL_GEARS[0], new_avg=4, new_std=0.4)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case study #2 : Initialization of the agent / engine with an LATE and VERY BRIEF anomaly on one device (GPU) \n",
    "We start with information of one benchmark (aka all implementations were run once)\n",
    "\n",
    "We run the workflow 100 times. \n",
    "\n",
    "During 30-40 runs there is an anomaly with one GPU which affects all opencl implementation that use that GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH NORMAL AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3)\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "sim.print_methods()\n",
    "\n",
    "sim.run_anomalous_simulations(iter_n=100, ano_start=30, ano_end=40, affected_gear=ALL_GEARS[0], new_avg=4, new_std=0.4)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH EXPONENTIALLY WEIGHTED AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1,exp=True)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2,exp=True)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3,exp=True)\n",
    "\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "sim.print_methods()\n",
    "\n",
    "sim.run_anomalous_simulations(iter_n=100, ano_start=30, ano_end=40, affected_gear=ALL_GEARS[0], new_avg=4, new_std=0.4)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case study #3 : Initialization of the agent / engine with an EARLY and LONG anomaly on one device (GPU)\n",
    "\n",
    "We start with information of one benchmark (aka all implementations were run once)\n",
    "\n",
    "We run the workflow 100 times. \n",
    "\n",
    "During 10-60 runs there is an anomaly with one GPU which affects all opencl implementation that use that GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH NORMAL AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3)\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "sim.print_methods()\n",
    "\n",
    "sim.run_anomalous_simulations(iter_n=100, ano_start=10, ano_end=60, affected_gear=ALL_GEARS[0], new_avg=4, new_std=0.4)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH EXPONENTIALLY WEIGHTED AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1,exp=True)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2,exp=True)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3,exp=True)\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "sim.print_methods()\n",
    "\n",
    "sim.run_anomalous_simulations(iter_n=100, ano_start=10, ano_end=60, affected_gear=ALL_GEARS[0], new_avg=4, new_std=0.4)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case study #4 : Initialization of the agent / engine with a LATE and LONG anomaly on one device (GPU)\n",
    "We start with information of one benchmark (aka all implementations were run once)\n",
    "\n",
    "We run the workflow 100 times. \n",
    "\n",
    "During 30-80 runs there is an anomaly with one GPU which affects all opencl implementation that use that GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH NORMAL AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3)\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "sim.print_methods()\n",
    "\n",
    "sim.run_anomalous_simulations(iter_n=100, ano_start=30, ano_end=80, affected_gear=ALL_GEARS[0], new_avg=4, new_std=0.4)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WITH EXPONENTIALLY WEIGHTED AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1 = dict(zip(ALL_GEARS,[1,1.5,3,3,3,3,3,4,10]))\n",
    "std_1 = dict(zip(ALL_GEARS,[0.1,0.15,0.3,0.3,0.3,0.3,0.3,0.4,1]))\n",
    "met_1 = Method('1',avg_1,std_1,exp=True)\n",
    "\n",
    "avg_2 = dict(zip(ALL_GEARS,[2,2,1,1,1,1,1,3,10]))\n",
    "std_2 = dict(zip(ALL_GEARS,[.2,.2,.1,.1,.1,.1,.1,.3,1]))\n",
    "met_2 = Method('2',avg_2,std_2,exp=True)\n",
    "\n",
    "avg_3 = dict(zip(ALL_GEARS,[3,3,3,3,3,3,3,3,3]))\n",
    "std_3 = dict(zip(ALL_GEARS,[0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]))\n",
    "met_3 = Method('3',avg_3,std_3,exp=True)\n",
    "\n",
    "\n",
    "sim = Simulator(met_1,met_2,met_3)\n",
    "sim.benchmark_all_methods(1)\n",
    "sim.print_methods()\n",
    "\n",
    "sim.run_anomalous_simulations(iter_n=100, ano_start=30, ano_end=80, affected_gear=ALL_GEARS[0], new_avg=4, new_std=0.4)\n",
    "sim.print_methods()\n",
    "sim.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanopyx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
